{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851f035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conda install -c conda-forge spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861fe58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4ae2e6-4d89-4dd2-9da8-16d8756ce686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from sklearn.metrics import silhouette_score\n",
    "from spacy import displacy\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37130179-5e8f-4ad3-8054-470f7fb51934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allen_Forrest | birthPlace | \"Fort Campbell, K...</td>\n",
       "      <td>Allen Forrest was born in Fort Campbell, KY an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Akron_Summit_Assault | ground | St._Vincent–St...</td>\n",
       "      <td>The ground of Akron Summit Assault is in St Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addis_Ababa_City_Hall | buildingStartDate | 19...</td>\n",
       "      <td>The Addis Ababa City Hall was built in 1961 an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALCO_RS-3 | builder | Montreal_Locomotive_Work...</td>\n",
       "      <td>The ALCO RS-3 has a V12 engine and is 17068.8 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Atlantic_City_International_Airport | operatin...</td>\n",
       "      <td>The Port Authority of New York and New Jersey,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  Allen_Forrest | birthPlace | \"Fort Campbell, K...   \n",
       "1  Akron_Summit_Assault | ground | St._Vincent–St...   \n",
       "2  Addis_Ababa_City_Hall | buildingStartDate | 19...   \n",
       "3  ALCO_RS-3 | builder | Montreal_Locomotive_Work...   \n",
       "4  Atlantic_City_International_Airport | operatin...   \n",
       "\n",
       "                                         target_text  \n",
       "0  Allen Forrest was born in Fort Campbell, KY an...  \n",
       "1  The ground of Akron Summit Assault is in St Vi...  \n",
       "2  The Addis Ababa City Hall was built in 1961 an...  \n",
       "3  The ALCO RS-3 has a V12 engine and is 17068.8 ...  \n",
       "4  The Port Authority of New York and New Jersey,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triplet_to_text = pd.read_csv('train_triplet_to_text_2.csv')\n",
    "train_triplet_to_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8bebd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Allen Forrest was born in Fort Campbell, KY an...\n",
       "1        The ground of Akron Summit Assault is in St Vi...\n",
       "2        The Addis Ababa City Hall was built in 1961 an...\n",
       "3        The ALCO RS-3 has a V12 engine and is 17068.8 ...\n",
       "4        The Port Authority of New York and New Jersey,...\n",
       "                               ...                        \n",
       "28381    Antwerp International Airport serves the city ...\n",
       "28382    Aaron Hunt has played for, Viktor Skrypnyk man...\n",
       "28383    There are 600 students at the Accademia di Arc...\n",
       "28384                  Alberto Teisaire is a Rear Admiral.\n",
       "28385    Hüseyin Bütüner and Hilmi Güner designed the B...\n",
       "Name: target_text, Length: 28386, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triplet_to_text[\"target_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2622e8c-17c5-4295-bda6-54d21773342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_tuples_dict = {}\n",
    "for idx,row in train_triplet_to_text.iterrows():\n",
    "    text_to_tuples_dict[row['target_text']] = row['input_text'].split(\"&&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa1266-1fda-4792-a5c2-cb76b7c79be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text  = str(text)\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59985eb2-7232-44b2-a82b-3a19e5b16aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = {} \n",
    "# dictionary format -- {text : {split_sentences: [ ] , tuples : []}}\n",
    "for text in tqdm(list(text_to_tuples_dict.keys()), position = 0, desc = \"Progress\"):\n",
    "    \n",
    "    split_sentences = split_into_sentences(text)\n",
    "    tuples = text_to_tuples_dict[text]\n",
    "    master_dict[text] = {'split_sentences': split_sentences,\n",
    "                        'tuples' : tuples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6310dc-85af-42e9-93cc-cf04a13d1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict[list(master_dict.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b492faa-5556-4e27-934c-f1cfb2a3b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def show_dependency_graph(doc, save_file = False):\n",
    "    print (\"{:<15} | {:<8} | {:<15} | {:<20}\".format('Token','Relation','Head', 'Children'))\n",
    "    print (\"-\" * 70)\n",
    "\n",
    "    for token in doc:\n",
    "      # Print the token, dependency nature, head and all dependents of the token\n",
    "      print (\"{:<15} | {:<8} | {:<15} | {:<20}\"\n",
    "             .format(str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children])))\n",
    "\n",
    "     # Use displayCy to visualize the dependency \n",
    "    img = displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})\n",
    "    if save_file:\n",
    "        output_path = Path(\"dependency_plot.svg\") # you can keep there only \"dependency_plot.svg\" if you want to save it in the same folder where you run the script \n",
    "        output_path.open(\"w\", encoding=\"utf-8\").write(img)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16bc00-34a4-4ce7-93b2-fbbe86ae92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for key in tqdm(list(master_dict.keys()), position = 0, desc = \"Progress : \"):\n",
    "    \n",
    "    split_sentences = master_dict[key]['split_sentences']\n",
    "    dependency_relations = []\n",
    "    for sentence in split_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        dependency_relations_sentence = [doc]\n",
    "        for token in doc:\n",
    "            dep_dict = {}\n",
    "            text = token.text\n",
    "            Relation = token.dep_\n",
    "            Head = token.head.text\n",
    "            Children = [child for child in token.children]\n",
    "#             print(text, Relation , Head, Children)\n",
    "            dep_dict['text'] = text\n",
    "            dep_dict['Relation'] = Relation\n",
    "            dep_dict['Head'] = Head\n",
    "            dep_dict['Children'] = Children\n",
    "            dependency_relations_sentence.append(dep_dict)\n",
    "        dependency_relations.append(dependency_relations_sentence)\n",
    "    master_dict[key]['dependency_relations'] = dependency_relations\n",
    "        \n",
    "#         show_dependency_graph(doc)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf4992-be37-4646-99ed-e58331db7ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_idx = 1\n",
    "master_dict[list(master_dict.keys())[sample_idx]]['dependency_relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f1f6b1-8554-413d-b03c-9ad396850f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dependency_graph(master_dict[list(master_dict.keys())[sample_idx]]['dependency_relations'][0][0],save_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57548ae4-32f7-42bc-95bc-4fa4601f3854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3c24e94",
   "metadata": {},
   "source": [
    "# Clustering Test on random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd69b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conda install -c conda-forge sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac7c09-9151-444b-8bb3-85eb0314267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1373bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dakshthapar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6e2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'A man is eating pasta.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'The baby is carried by the woman',\n",
    "          'A man is riding a horse.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.',\n",
    "          'A cheetah is running behind its prey.',\n",
    "          'A cheetah chases prey on across a field.'\n",
    "            ]\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_sent = []\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f98950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bd660",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert_model.encode(sentences)\n",
    "print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_embeddings), sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform kmean clustering\n",
    "num_clusters = 5\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(sentence_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5ceb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca142c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017fefa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "463734c5",
   "metadata": {},
   "source": [
    "# Clustering Test on WebNLG data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 28386/28386 [00:02<00:00, 11010.57it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences=list(train_triplet_to_text[\"target_text\"])\n",
    "\n",
    "# Tokenizaton of each document\n",
    "tokenized_sent = []\n",
    "for s in tqdm(sentences):\n",
    "    try:\n",
    "        tokenized_sent.append(word_tokenize(s.lower()))\n",
    "    except:\n",
    "        pass\n",
    "sentence_embeddings = sbert_model.encode(sentences)\n",
    "print(len(sentence_embeddings), sentence_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c65f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 50\n",
    "clustering_model = KMeans(n_clusters=num_clusters,random_state=0).fit(sentence_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb0a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clust_num=[]\n",
    "clust_len=[]\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    clust_num.append(i+1)\n",
    "    clust_len.append(len(cluster))\n",
    "\n",
    "plt.plot(clust_num,clust_len)\n",
    "plt.xlabel('Cluster number') \n",
    "plt.ylabel('Cluster size') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70285b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = []\n",
    "for i in range(300,1000,200):\n",
    "    range_n_clusters.append(i)\n",
    "silhouette_avg = []\n",
    "\n",
    "for num_clusters in range_n_clusters:\n",
    "    # initialise kmeans\n",
    "    \n",
    "    clustering_model = KMeans(n_clusters=num_clusters,random_state=0).fit(sentence_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_  \n",
    "\n",
    "    # silhouette score\n",
    "    silhouette_avg.append(silhouette_score(sentence_embeddings, cluster_assignment))\n",
    "    \n",
    "plt.plot(range_n_clusters,silhouette_avg,'bx-')\n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Silhouette score') \n",
    "plt.title('Silhouette analysis For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62436a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentence_embeddings),sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75043181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "S = StandardScaler().fit_transform(sentence_embeddings)\n",
    "principalComponents = pca.fit_transform(S)\n",
    "principalComponents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def5840",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932833da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b68968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(principalComponents[:, 0], principalComponents[:, 1], c=cluster_assignment, s=50, cmap='rainbow')\n",
    "# plt.legend()\n",
    "centers = clustering_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7532c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652283c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
